## S3->SQS->Lambda.

Suppose we want to trigger a lambda function when an image file is uploaded to the S3 bucket, 
However, we want to have only a few concurrent instances of the function when the upload count is high. We can use an SQS queue to batch the upload event messages before triggering a function instance.

![][arch1]

In `lib/eda-app-stack.ts`, add the following code after the bucket creation:
~~~ts
    // Integration infrastructure

  const queue = new sqs.Queue(this, "img-created-queue", {
      receiveMessageWaitTime: cdk.Duration.seconds(10),
    });

    // Lambda functions

    const processImageFn = new lambdanode.NodejsFunction(
      this,
      "ProcessImageFn",
      {
        // architecture: lambda.Architecture.ARM_64,
        runtime: lambda.Runtime.NODEJS_18_X,
        entry: `${__dirname}/../lambdas/processImage.ts`,
        timeout: cdk.Duration.seconds(15),
        memorySize: 128,
      }
    );

    // Event triggers

    imagesBucket.addEventNotification(
      s3.EventType.OBJECT_CREATED,
      new s3n.SqsDestination(queue)
    );

    const newImageEventSource = new events.SqsEventSource(queue, {
      batchSize: 5,
      maxBatchingWindow: cdk.Duration.seconds(10),
    });

    processImageFn.addEventSource(newImageEventSource);

    // Permissions

    imagesBucket.grantRead(processImageFn);

    // Output
    
    new cdk.CfnOutput(this, "bucketName", {
      value: imagesBucket.bucketName,
    });
~~~
The code above configures the S3 bucket to write a notification message to an SQS queue when new objects (files) are added. This queue is attached to a lambda function as a trigger, which results in the lambda handler receiving a message for each file/object uploaded to the bucket. The messages include the identity details for the uploaded objects to allow the lambda function to access the image files. We grant the function read permission to the bucket to access the files successfully.

Create the file `lambdas/processImage.ts` and add the following code:
~~~ts
/* eslint-disable import/extensions, import/no-absolute-path */
import { SQSHandler } from "aws-lambda";
import {
  GetObjectCommand,
  GetObjectCommandInput,
  S3Client,
} from "@aws-sdk/client-s3";

const s3 = new S3Client();

export const handler: SQSHandler = async (event) => {
  console.log("Event ", event);
  for (const record of event.Records) {
    const recordBody = JSON.parse(record.body);
    if (recordBody.Records) {
      console.log("Record body ", JSON.stringify(recordBody));
      for (const messageRecord of recordBody.Records) {
        const s3e = messageRecord.s3;
        const srcBucket = s3e.bucket.name;
        // Object key may have spaces or unicode non-ASCII characters.
        const srcKey = decodeURIComponent(s3e.object.key.replace(/\+/g, " "));
        let origimage = null;
        try {
          // Download the image from the S3 source bucket.
          const params: GetObjectCommandInput = {
            Bucket: srcBucket,
            Key: srcKey,
          };
          origimage = await s3.send(new GetObjectCommand(params));
          // Process the image ......
        } catch (error) {
          console.log(error);
        }
      }
    }
  }
};

~~~
Note that S3 bundles the file upload event messages it writes to SQS, and SQS batches the messages it sends to the lambda function. Therefore, the function needs nested for-loops to access the individual file upload events generated by S3. The function downloads the image files using the S3 client's GetObjectCommand object for local processing (more on the processing later).

Deploy the updated stack:
~~~bash
$ cdk deploy
~~~
To test the stack, upload an image using the CLI:
~~~bash
$ aws s3 cp ./images/sunflower.jpeg  s3://your_bucket_name/image1.jpeg
e.g.
$ aws s3 cp ./images/sunflower.jpeg  s3://simpleappstack-images9bf4dcd5-tc5q9f314rn6/image1.jpeg
~~~
In the management console, go to CloudWatch --> Log Groups --> Select the group for /aws/lambda/EDAStack-ProcessImageFnxxxxx --> Select the log Stream for the group. The event passed to the handler contains a batch (array) of messages retrieved from the SQS queue, and the body of each message is a stringified JSON structure, as illustrated below:

![][event]

The stringified JSON contains a batch (array, possibly of size one) of file upload event messages from the S3 bucket. A message includes identification details of the file uploaded, as shown below:

![][message]

Commit this work:
~~~bash
$ git add -A
$ git commit -m "Signup resource"
~~~

[arch1]: ./img/arch1.png
[event]: ./img/event.png
[message]: ./img/message.png

[pathparameters]: ./img/pathparameters.png

