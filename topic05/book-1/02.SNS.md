## SNS Fan out pattern.

Suppose we want to send an email message when an image upload event occurs, in addition to processing the image. Rather than combine the emailing functionality with the image processing in a single lambda, keeping them seperate is a better design (separation of concerns principle) and allows parallel processing. The updated design for our app looks as follows:

![][arch]

Now, S3 publishes image upload events on an SNS topic/channel. The topic has two SQS subscribers, each feeding batches of upload messages to their respective lambda function consumer.

In `lib/eda-app-stack.ts`, make the following changes:

+ Rename the current queue to imageProcessQueue:
~~~ts
    const imageProcessQueue = new sqs.Queue(this, "img-created-queue", {
      receiveMessageWaitTime: cdk.Duration.seconds(10),
    });
~~~
+ Add an SNS topic (after the queue declaration):
~~~ts
    const newImageTopic = new sns.Topic(this, "NewImageTopic", {
      displayName: "New Image topic",
    }); 
~~~
+ Change the S3 event notification destination type to SNS:  
~~~ts
    imagesBucket.addEventNotification(
        s3.EventType.OBJECT_CREATED,
        new s3n.SnsDestination(newImageTopic)  // Changed
    );
~~~
+ Set the queue as a subscriber to the SNS topic:
~~~ts
    newImageTopic.addSubscription(
      new subs.SqsSubscription(imageProcessQueue)
    );
~~~
Messages written to an SNS topic or SQS queue) are stored in stringified form. Therefore, a lambda function that consumes messages that pass from SNS on to  SQS must perform two parsing operations. In `lambdas/processImage.ts`, replace the code with the following:
~~~ts
/* eslint-disable import/extensions, import/no-absolute-path */
import { SQSHandler } from "aws-lambda";
import {
  GetObjectCommand,
  PutObjectCommandInput,
  GetObjectCommandInput,
  S3Client,
  PutObjectCommand,
} from "@aws-sdk/client-s3";

const s3 = new S3Client();

export const handler: SQSHandler = async (event) => {
  console.log("Event ", JSON.stringify(event));
  for (const record of event.Records) {
    const recordBody = JSON.parse(record.body);        // Parse SQS message
    const snsMessage = JSON.parse(recordBody.Message); // Parse SNS message

    if (snsMessage.Records) {
      console.log("Record body ", JSON.stringify(snsMessage));
      for (const messageRecord of snsMessage.Records) {
        const s3e = messageRecord.s3;
        const srcBucket = s3e.bucket.name;
        // Object key may have spaces or unicode non-ASCII characters.
        const srcKey = decodeURIComponent(s3e.object.key.replace(/\+/g, " "));
        let origimage = null;
        try {
          // Download the image from the S3 source bucket.
          const params: GetObjectCommandInput = {
            Bucket: srcBucket,
            Key: srcKey,
          };
          origimage = await s3.send(new GetObjectCommand(params));
          // Process the image ......
        } catch (error) {
          console.log(error);
        }
      }
    }
  }
};
~~~
We will add the emailing functionality shortly, but for now, redeploy the stack (cdk deploy). Test it by uploading an image - we will use the same image but give it a different S3 key:
~~~bash
Â£ aws s3 cp ./images/sunflower.jpeg  s3://your_bucket_name/image2.jpeg
~~~
In Cloudwatch, find the stream generated by the lambda handler for the above upload:

![][sns]

![][s3]

Commit this work:
~~~bash
$ git add -A
$ git commit -m "Started Fan out for S3 image upload."
$ git push origin master
~~~

[arch]: ./img/arch.png
[s3]: ./img/s3.png
[sns]: ./img/sns.png


