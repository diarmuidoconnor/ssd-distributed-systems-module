## S3->SQS->Lambda.

Suppose we want to trigger a lambda function when an image file is uploaded to the S3 bucket, but still limit the number of concurrent running instances of the function when the upload frequency is high. A solution to satisfy this scenario is to insert an SQS queue between the bucket and lambda function. The SQS service logs the upload events generated by S3, and groups them into batched when the Lambda service polls SQS.

![][arch1]

In `lib/eda-app-stack.ts`, place the following code after the bucket creation:
~~~ts
  // Integration infrastructure

  const queue = new sqs.Queue(this, "img-created-queue", {
      receiveMessageWaitTime: cdk.Duration.seconds(5),
    });

    // Lambda functions

    const processImageFn = new lambdanode.NodejsFunction(
      this,
      "ProcessImageFn",
      {
        runtime: lambda.Runtime.NODEJS_22_X,
        entry: `${__dirname}/../lambdas/processImage.ts`,
        timeout: cdk.Duration.seconds(15),
        memorySize: 128,
      }
    );

    // S3 --> SQS
    imagesBucket.addEventNotification(
      s3.EventType.OBJECT_CREATED,
      new s3n.SqsDestination(queue)
    );

   // SQS --> Lambda
    const newImageEventSource = new events.SqsEventSource(queue, {
      batchSize: 5,
      maxBatchingWindow: cdk.Duration.seconds(5),
    });

    processImageFn.addEventSource(newImageEventSource);

    // Permissions

    imagesBucket.grantRead(processImageFn);

    // Output
    
    new cdk.CfnOutput(this, "bucketName", {
      value: imagesBucket.bucketName,
    });
~~~
The code above configures the S3 service to notify the SQS service when an object is uploaded, and SQS logs this event in the queue. This queue is the event source (trigger) for the lambda function. The event received by the function will contain a batch of SQS messages related to upload events. Each messages includes the identity of the uploaded objects, enabling the lambda function to access it, if necessary.

Create the file `lambdas/processImage.ts` and add the following code:
~~~ts
/* eslint-disable import/extensions, import/no-absolute-path */
import { SQSHandler } from "aws-lambda";
import {
  GetObjectCommand,
  GetObjectCommandInput,
  S3Client,
} from "@aws-sdk/client-s3";

const s3 = new S3Client();

export const handler: SQSHandler = async (event) => {
  console.log("Event ", JSON.stringify(event));
  for (const record of event.Records) {
    const recordBody = JSON.parse(record.body);
    if (recordBody.Records) {
      console.log("Record body ", JSON.stringify(recordBody));
      for (const messageRecord of recordBody.Records) {
        const s3e = messageRecord.s3;
        const srcBucket = s3e.bucket.name;
        // Object key may have spaces or unicode non-ASCII characters.
        const srcKey = decodeURIComponent(s3e.object.key.replace(/\+/g, " "));
        let origimage = null;
        try {
          // Download the image from the S3 source bucket.
          const params: GetObjectCommandInput = {
            Bucket: srcBucket,
            Key: srcKey,
          };
          origimage = await s3.send(new GetObjectCommand(params));
          // Process the image ......
        } catch (error) {
          console.log(error);
        }
      }
    }
  }
};
~~~
The above function contains nested for-loop constructs. The outer loop iterates over the batch of messages received from SQS; the inner loop deals with the possibility (very rare) of multiple file upload events in a single SQS message. For illustration purposes, the function downloads each file locally for possible processing (not relevant here).

Deploy the updated stack:
~~~bash
$ cdk deploy
~~~
To test the stack, upload an image using the AWS CLI:
~~~bash
$ aws s3 cp ./images/sunflower.jpeg  s3://your_bucket_name/image1.jpeg
e.g.
$ aws s3 cp ./images/sunflower.jpeg  s3://simpleappstack-images9bf4dcd5-tc5q9f314rn6/image1.jpeg
~~~
In the management console, go to CloudWatch --> Log Groups --> Select the group for /aws/lambda/EDAStack-ProcessImageFnxxxxx --> Select the most recent log stream. Notice in the console log of the event parameter, the body of each SQS message is a stringified JSON structure (it must be parsed before processing), as illustrated below:

![][event]


The body of each message is also an array and represents a set of S3 events/messages - usually this array has only one element. Each message includes identification details of the file uploaded, as shown below:

![][message]

Commit this work:
~~~bash
$ git add -A
$ git commit -m "Signup resource"
$ git push origin master
~~~

[arch1]: ./img/arch1.png
[event]: ./img/event.png
[message]: ./img/message.png
[pathparameters]: ./img/pathparameters.png

